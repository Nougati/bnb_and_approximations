---
 header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage[linesnumbered,ruled]{algorithm2e}
  - \usepackage{accents}
  - \newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
 output: pdf_document
 bibliography: literaturereviewv2.bib
---

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{titlepage}
  \centering
  {\scshape\LARGE Monash University \par}
  \vspace{1cm}
  {\scshape\Large Literature Review \par}
  \vspace{1.5cm}
  {\huge Implicit enumeration with dual bounds from approximation algorithms \par}
  \vspace{2cm}
  {\Large Nelson Frew \par}
  \vfill
  supervised by \par
  Dr. Pierre Le Bodic \& Arthur Mah√©o
  \vfill
  {\large \today\par}
\end{titlepage}

# Abstract

Implicit enumeration refers to a divide-and-conquer problem solving strategy where we can enumerate through a problem's solutions without specifically finding each solution.
 This has broad uses within many fields of Computer Science, one such example being in Mixed-Integer Programming's (MIP) Branch-and-Bound (B&B).
 To omit evaluation of solutions from our search, we analyse them with regards to bounds established on where the optimal value can reside, known as dual bounds.
 Traditionally, bounds are found by relaxing the problem formulation so that it can be solved efficiently, however such approaches may perform arbitrarily poorly according to problem type.
 In this literature review, we describe the research context with particular reference to MIP and B&B, linking with relevant research in the area that leads to our new approach of deriving dual bounds: approximation algorithms.

\newpage
\tableofcontents
\newpage

# Introduction


Discrete optimisation studies methods to solve optimisation problems involving discrete decision making.
 For example, discrete optimisation addresses a large range of problems, including scheduling, shortest path and cutting stock problems.
 A widely used method for solving such problems is Mixed-Integer Programming: a set of modelling and solving techniques for problems where at least one variable must be integer. In this report, we consider maximisation problems where all constraints are linear. Their formulation as Mixed-Integer Programs (MIPs) is as follfows:

\begin{equation*}
\begin{array}{ll@{}ll}
\text{maximse}  & \displaystyle cx+hy &\\
\text{subject to}  & \displaystyle Ax + Gy \leq b \\
                   &x \geq 0 \text{ integral}\\
                   &y \geq 0  
\end{array}
\end{equation*}

 In this case, removing the integrality constraint results in a Linear Program (LP).
 This is called an LP relaxation.
 While a MIP is an NP-hard problem, solving an LP can be done in polynomial time \cite{InteriorPoint}.

The state of the art method for solving MIPs is the Branch-and-Bound (B&B), which uses the LP relaxations to achieve *implicit enumeration*.
 First, the integer constraints are removed, and subproblems are created by constraining individual variables in some way.
 These subproblems are solved as LPs which provide an upper bound on the integer solution.
 By using the solution of a subproblem as an upper bound, we can deduce if a better solution can be found by constraining it further; if its solution is lower than our lower bound on the original MIP, we can safely remove this problem from our search.

If optimality is not required, exact methods such as the B&B can be replaced with Approximation Algorithms (AAs).
 An AA for a problem is an algorithm guaranteed to provide a value within a constant factor of the optimal value.
 Because optimality does not need to be proven, AAs can be run much faster than exact solvers.
 When maximising, the guarantee on the optimal solution is a lower bound, and one can analytically derive an upper bound on the optimal value from this information.
 Given this, one can construct a B&B scheme using only AAs to bound subproblems.
 We present a premise to explore such an approach by exploiting information gained about the problem after the appoximation has been found.

In this literature review, we will address the context of all the concepts discusses so far. We first describe the B&B and discuss research in improving it, and then introduce the context of AAs with the Knapsack Problem (KP) as an example. In the final sections, we show how this relates to our work and conclude with the project's next steps.

#Branch-and-bound 

## Overview 

Solving a MIP with B&B involves establishing *primal* and *dual* bounds on the optimal value.
It starts with relaxing the pronlem entirely, then explore partial assignments by constraining variables to given values.
Each partial assignment is a relaxation of the problem. Therefore, successive partial assignments will yield a monotonically incresing value.
Thus, if the value of such an assignment (dual bound at a node) exceeds our primal bound, we can stop the exploration (boungind or pruning). Otherwise, we keep exploring.
In case we have an integer solution, we can update our primal bound.

The B&B algorithm, as described by \cite{Conforti}, is shown below: let $(x^i, y^i)$ be the optimal solution to linear program $LP_i$, $(x*, y*)$ denote an optimal solution and $z*$ the optimal solution of the MIP, and let $\mathcal{L}$ denote the list of nodes yet to be solve (i.e. not pruned nor branched on)\;

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A Mixed-Integer Program.}
  \KwResult{The optimal solution value}
  Set $\mathcal{L}:={N_0}$, set $\ubar z := -\infty$, set  $(x*, y*):=\emptyset$\;
  \While{$\mathcal{L}$ is not empty}
  {
  Choose a node $N_i$ from $\mathcal{L}$, and delete it from $\mathcal{L}$\;
  Solve $LP_i$\;
  \If{$LP_i$ is infeasible}
  {
  Go to step 3\;
  }
  \Else
  {
  Let $(x^i, y^i)$ be an optimal solution of $LP_i$ and $z_i$ be its objective value\:
  }
  \eIf{$z_i \leq \ubar z $}
  {
  Go to step 3\;
  }
  {
  \If{$(x^i, y^i)$ is feasible to the MIP}
  {
  Set $\ubar z$ $:=$ $z_i$\;
  Set $(x^*, y*)$ $:=$ $(x^i , y^i)$\;
  Go to step 3\;
  }
  }
  \Else
  {
  From $LP_i$ construct $k \geq 2$ linear programs $K_{i1}, \dots, LP_{ik}$ with smaller feasible regions whose union does not contain $(x^i, y^i)$, but contains all the solutions of $LP_i$ with $x \in \mathbb{Z}$\;
  Add the corresponding new nodse $N_{i1},\dots, N_{ik}$ to $\mathcal{L}$ and go to step 3.
  }

  Return $\ubar z$\;
  }
\caption{Branch and Bound algorithm for Mixed-Integer Programming}
\end{algorithm}


From this descripion, we still need to determine: the best variable to branch on, the best way to relax a solution, and which partial solution to solve next.
 These questions are in fact non-trivial and active research areas within Mixed-Integer Programming.
 We briefly discuss research which aims to improves the performance of a B&B in terms of its *branching*, *bounding*, and *searching* strategies.

## Branching strategies

To partially enumerate solutions, we need to decided which variables to constrain.
 A basic strategy is to branch on the variable with the largest fractional component, however \cite{Achterberg} has showed this to be as bad as random selection. 
 Tomlin \cite{Tomlin} introduced the concept of *use penalties* to inform the branching process, where "up penalties" and "down penalties" were calculated for each variable based on their respective fractional components, the variable with the largest penalty in either direction was chosen.
 Mitra \cite{Mitra} conducted computational experiments which demonstrated the limited use of such approaches, but acknowledged that the similar strategies proposed by \cite{Benichou}, involving "pseudocosts" on variables, may advantages.
 To organise the numerous branching methods, \cite{Morrison} proposed two broad categorisations of strategies: *binary* and non-binary, or *wide*.
 Each category of branching has benefits and drawbacks depending on tree depth and problem type.
 There has been concentrated research on branching strategies for many years, and it is still an active area.
 A detailed survey of the literature and the current questions in research are provided in \cite{Morrison}, where the reader is directed for further reading.


# Methods of bounding an optimal solution

Another important component of the B&B's success is the method of bounding partially enumerated solutions, which are relaxed forms of the original problem.
 By removing constraints from the original problem, the relaxed feasible region will naturally subsume the region of the original problem.
 We want to optimise the objective of this relaxation in order to bound the optimal solution. 
 By finding better relaxations, we minimise the size of the search tree.

Within the Mixed-Integer Programming field, there have been various methods to achieve this: LP relaxations, Semidefinite Programming relaxations (see \cite{Vendenberghe, Lovasz}), and Lagrangian relaxations \cite{Geoffrion}.
 Solving Linear Programs has been done efficiently ever since the introduction of the Simplex by Dantzig (1947) \cite{Dantzig}.
 However LP relaxations can provide weak bounding: for example, LP relaxations for Vertex Cover are generally far from the optimal value.  

## Extensions, improvements, and related techniques 

State-of-the-art solvers for MIPs often combine B&B strategies with various extensions to improve performance.
 One common extension to the method includes using cutting-planes \cite{Gomory} to produce the Branch-and-Cut algorithm.
In the cutting plane approach, introduced by Johnson et al. (1954) \cite{DantzigFulkersonJohnson} and extended to general IPs by Gomory (1958) one solves a linear relaxation of a program and formulates a constraint (a cutting plane) that separates this solution from the search space. 
A Branch-and-Cut \cite{PadbergRinaldi1, PadbergRinaldi2} is simply a B&B approach that can decide to add these cutting planes instead of branching on a solution.
For a more detailed treatment, the reader is referred to \cite{Conforti}.
Other key methods for extending the B&B are presolving techniques \cite{Mahajan} and primal heuristics \cite{berthold2006primal}.

Another related concept is the idea of *warm starting a solution* in linear programming.
 In short, warm starting is a method to exploit information gained about the problem from previous computations to inform future ones.
 One simple method to do this is to use previous computations in finding a starting primal bound in a B&B.
 Ralphs and Guzelsoy (2006) \cite{Ralphs} made links to this concept specifically in regards to Integer Programming.


# Approximations

## Overview

Within the study of computationally difficult problems, the field of Approximation Algorithms (AAs) arose to provide fast approximations where no efficient optimal solution algorithm was known.
 Specifically, AA research involves finding algorithms with solution values provably within a factor of the optimal value.
 We refer to an algorithm as an $\alpha$-approximation algorithm if, regardless of instance, it will return a value within a constant factor $alpha$ of the optimal.
 For a detailed treatment, we refer the reader to the book by Vazirani (2001) \cite{Vazirani}.

## Case study: Finding an approximation scheme for Knapsack

To illustrate the nature of AA research we demonstrate with a description of an approximation to the classic problem in computer science, the Knapsack Problem (KP).
 KP is defined as follows: given $n$ items with associated weights $w_i$ and values $v_i$, and Knapsack capicity $W$, our program is

\begin{equation*}
\begin{array}{ll@{}ll}
\text{minimize}  & \displaystyle\sum\limits_{i=1}^{n} v_{i}&x_{i} &\\
\text{subject to}& \displaystyle\sum\limits_{i=1}^{n}     &w_{i} x_{i} \leq W, \text{ and } x_{i} \in \{0,1\}\\
                 &  
\end{array}
\end{equation*}

To solve this problem, we refer to the Dynamic Programming (DP) algorithm as described by \cite{Vazirani}: let $P$ be the value of the most valuable object in the item set, and let $A(i,p)$ be the minimal weight of the solution to the KP with only the first $i$ items available, where the total value is exactly $p$.
 If no such set exists \footnote{that is, the first $i$ items cannot yield a value of $p$ and so cannot have an associated minimal weight}, $A(i,p) = \infty$.
 The DP recurrence can be defined as follows:

\begin{equation}
  A(i+1,p) = \begin{cases}
    \text{min} \{A(i,p), w_{i+1} + A(i,p-v_{i+1})\} \text{, if }v_{i+1}<p \\
    A(i+1, p) = A(i,p) \text{ otherwise}
  \end{cases}
\end{equation}

The solution is given, then, by finding max$\{p | A(n,p) \leq W\}$, providing exact computation in $O(n^2 P)$, as shown by \cite{Vazirani}.
 
From this DP algorithm, we now derive a *Fully Polynomial Time Approximation Scheme* (FPTAS) for the KP, by introducing an error parameter $\epsilon$.
 We call an algorithm an FPTAS if its runtime is bounded by a polynomial both in its instance size and $1/\epsilon$.
 In the case of the KP, we construct an FPTAS by truncating values of the profits, parameterised by $\epsilon$.

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A KP problem instance, and error parameter $\epsilon$.}
  \KwResult{The Weight and value pair of the optimal solution}
  Given $\epsilon$, let $K = \frac{\epsilon P}{n}$\;
  For each object $i$, define adjusted values $v'=\left \lfloor \frac{v_i}{K}\right \rfloor$\;
  Run a DP with these adjusted item values\;
  Return the approximation solution set, $S'$, provided by the DP\;
\caption{FPTAS for Knapsack}
\end{algorithm}

This algorithm guarantees that its solution is at least $1-\epsilon)\cdot z_O$, for optimal value $z_O$.

 The proof for this result is as follows: let $z_{S'}$ be the value of the solution set $S'$ obtained from the FPTAS, and $z_O$ be the value of the optimal solution set $O$. 
 Also, let $z'_{S'}$ be the value of the solution set with adjusted/truncated values value the FPTAS's solution set $S'$, and $z'_O$ be the optimal solution set with similarly adjusted value for all items. 

Because of the flooring operation truncating off at most $K$ we know that the difference between $z_{O}$ and $K \cdot z'_{O}$ would be at most $nK$.
 Furthermore, since $z'_{S'}$ is the optimal value for the adjusted profits, $z'_{O}$ cannot exceed it.
 So,

$$z_{O} - nK \leq K \cdot z'_O \leq K \cdot z'_{S'} \leq z_{S'} \leq z_O$$ 

Since $K = \frac{\epsilon P}{K}$, 

$$z_O - nK = z_O - \epsilon P$$ 

Finally, since $P \leq z_O$,

$$(1-\epsilon)\cdot z_O \leq z_{S'} \leq z_O$$

For further details and the proof associated with the time complexity of this scheme, refer to \cite{Vazirani}.

# Problem structure and the general LP formulation

An important observation of the FPTAS for KP is that it is achieved specifically by exploiting the problem structure.
 It is for this reason that LP programming can perform arbitrarily badly: it is inherently a general formulation.
 Further, there are algorithms where solutions are just easier to obtain combinatorially than with LP formulations.
 For example, finding the maximum matching of a graph: the combinatorially oriented Blossom Algorithm performs better than formulating an LP with blossom inequalities (CITE).
 The main drawback to LP is that there is no provable guarantee on solution quality.

# Relating approximations to LP values

As far as we know, the sole dedicated treatment that has been given to both AAs and LPs within the scope of implicit enumeration was provided by Wolsey (1980) \cite{Wolsey}.
 Wolsey provided a general analysis technique for relating AAs to LP relaxation solutions. 
 The objective was to reflect the relation between primal and dual bounds in a B&B.
 For an approximate solution value $Z^H$, and LP relaxation solution value $Z^{LP}$, he derived inequalities of the form "$Z^H \leq r Z^{LP} + s$ for $r\geq 1$".
 This analysis was extended to formulations on the Bin Packing Problem, Longest Undirected Hamiltonian Tours, Minimum Length Eularian Tours and the Chinese Postman Problem.
 Wolsey then showed that it was possible to achieve implicit enumeration in the form of a Branch-and-Bound algorithm.
 Specifically, he showed that AAs, when used in this way, would monotonically approach the value of the relaxation as the level of enumeration increased.
 The product is the only implicit enumeration scheme leveraging AA guarantees to find an optimal value that we are aware of.

# Implicitly enumerating with approximations

With the work from Wolsey, we can see that AAs can be used to achieve implicit enumeration by using an LP relaxation which operates as both a dual bound and as a point of convergence for the solution values of the AAs.
 However, we can omit the use of LP entirely by observing the following properties from the analysis on the KP where we saw:

$$(1-\epsilon) \cdot z_O \leq z_{S'} \leq z_O$$

Since 

$$(1-\epsilon) z_O \leq z_O - nK \leq K \cdot z'_{S'} $$

We find upper and lower bounds on the optimal value:

$$z_O \in [z_{S'}, K \cdot z'_{S'} + nK]$$

To use these within a branch and bound, the psuedocode is as follows:

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A MIP instance, and an AA for the given problem.}
  \KwResult{The Weight and value pair of the optimal solution}
  Let $\bar z :=$ approximate solution\;
  \While{$\mathcal{L}$ is not empty}
  {
  Choose a node $N_i$ from $\mathcal{L}$, and delete it from $\mathcal{L}$\;
  Run the AA, deriving upper bound (UB) and lower bound (LB) in the process\;
  \If{UB $\leq \bar z$}
  {
  Go to step 3\;
  }
  \Else
  {
  \If{LB$>\bar z$}
  {
  $\bar z := $ LB\;
  }
  }
  \If{for $N_i$'s parent's UB, $UB_p$, $UB > UB_p$}
  {
  Set $UB := UB_p$
  }
  \Else
  {
  Choose a variable to branch on, then generate and enqueue new child nodes;\
  }
  Return $\ubar z$\;
  }
\caption{Branch and Bound with Approximation Algorithms}
\end{algorithm}

# Conclusion

In this document we have overviewed the relevant components to understanding the relevant questions posed by using AAs to find dual bounds. We have seen that using such mechanisms allow us to get Dual Bounds with guarantee, and the steps that follow relate to seeing how we can use these components in light of previous research in MIP. There are several questions that remain: what searching and branching strategies does such an approach benefit from? How can we use the concept of Warm Starting in tandem with such components as the DP's that are computed to get dual bounds? How does such an approach compare to current state of the art methods?

Importantly, we hope to build on the work done by Wolsey and help to attract attention to find more applications for AAs in the future.

\bibliography{literaturereviewv2}
\bibliographystyle{plain}
