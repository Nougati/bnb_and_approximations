---
title: '(DRAFT) Implicit enumeration with dual bounds obtained from approximation algorithms'
author: Nelson Frew
header-includes:
 - \usepackage{amsmath}
 - \usepackage{amssymb}
 - \usepackage{amsthm}
 - \usepackage[linesnumbered,ruled]{algorithm2e}
 - \usepackage{accents}
 - \newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
 - \newtheorem{theorem}{Theorem}
 - \newtheorem{lemma}{Lemma}
output:
  pdf_document:
    number_sections: true
bibliography: project_overview_draft.bib
---

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\section{Introduction}

\subsection{Background}

The field of discrete optimisation (CITE) studies problems that model discrete
 decision making. 
Mixed-Integer Programming refers to a set of modelling and solving techniques for
 such discrete problems (CITE) by formulating them as Mixed-Integer Programs
 (MIPs). 
For many problems, this approach is the most efficient known method to find a
 solution. 
The standard MIP formulation, for maximisation, is as follows:

\begin{equation*}
\begin{array}{ll@{}ll}
\text{maximise}  & \displaystyle cx+hy &\\
\text{subject to}  & \displaystyle Ax + Gy \leq b \\
                   &x \geq 0 \text{ integral}\\
                   &y \geq 0
\end{array}
\end{equation*}

Where $x$ and $y$ are the sets of decision variables that we assign values to
 with the purpose of maximising the objective function $cx+hy$.
Generally, objective functions are constrained by linear inequalities, and
 variables can be specified to be integer or continuous. 

Optimisation problems with linear constraints and continuous variables are
 known as Linear Programs (LPs), which are both solvable in polynomial time
 and very efficiently in practice (CITE).
In this report, we consider maximisation problems whose constraints are all
 linear.
MIPs, on the other hand, are characterised by having one or more variables
 constrained to be integer, generally resulting in an NP-Hard problem (CITE). 
By removing the integer constraints on variables in a MIP, we obtain its
 associated *LP relaxation*.

Because the feasible region of continuous solutions naturally subsumes the set
 of feasible solutions for integers, we know, for optimal value for the LP
 relaxation $z_{LP}$ and optimal value to the MIP $z$, that

\begin{align*}
  z \leq z_{LP}
\end{align*}

Since $z_{LP}$ is at least as large as our optimal value, $z$ is provided with
 an upper bound, which we refer to as the *dual bound*.
The state-of-the-art method for solving MIPs uses this relationship to search
 for what are known as *primal solutions* that are feasible to the MIP,
 and employs dual bounds to implicitly enumerate through these. 
We find primal solutions by forming *subproblems*, constrained versions of the
 original problem, and solving the LP relaxation of these. 
If the solved LP relaxation is feasible to the original MIP, then we have found
 a primal solution, and so have an associated lower bound on our optimal value,
 which we refer to as the *primal bound*.

The process of choosing a variable to constrain is known as *branching*, and is
 generally informed by information obtained from the LP relaxation's solution. 
Branching can form a number of subproblems, which are all considered *branches*
 of the parent problem.
If we find that a given subproblem's dual bound is lower than the MIP's primal
 bound, then we can safely *prune* this branch as we know further evaluation
 and constraining of this subproblem cannot yield a better primal solution.
Otherwise, if this subproblem has a dual bound above our primal bound, we
 continue constraining and solving the relaxation until a new feasible solution
 is found, or a further constrained subproblem of this is pruned.

This constitutes the core of the Branch and Bound approach, which we formally
 describe below: let $(x^i, y^i)$ be $x$ and $y$ values associated with the
 optimal solution to linear program $LP_i$, $(x^*, y^*)$ denote an optimal
 solution to the MIP; $\ubar z$ denote the lower bound on the optimal value
 and $z^*$ the optimal solution of the MIP, and let $\mathcal{L}$ denote the
 list of nodes of the Branch and Bound tree yet to be solved (i.e. not pruned
 nor branched on).

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A Mixed-Integer Program.}
  \KwResult{The optimal solution value, and the associated solution}
  Set $\mathcal{L}:={N_0}$, set $\ubar z := -\infty$, set  $(x^*, y^*):=\emptyset$\;
  \While{$\mathcal{L}$ is not empty}
  {
  Select a node $N_i$ from $\mathcal{L}$, by some node selection scheme, deleting
   it from $\mathcal{L}$\;
  Solve $LP_i$ to get solution value $z_{i}$ and $(x^i, y^i)$ if it exists\;
  \If{$LP_i$ is infeasible, i.e. $z_i = -\infty$}
  {
  Go to step 2\;
  }
  \Else
  {
  Let $(x^i, y^i)$ be an optimal solution of $LP_i$ and $z_i$ be its objective value\:
  }
  \eIf{$z_i \leq \ubar z $}
  {
  Go to step 2\;
  }
  {
  \If{$(x^i, y^i)$ is feasible to the MIP}
  {
  Set $\ubar z$ $:=$ $z_i$\;
  Set $(x^*, y^*)$ $:=$ $(x^i , y^i)$\;
  Go to step 2\;
  }
  }
  \Else
  {
  Branching: From $LP_i$ construct 2 linear programs $LP_{i1}, LP_{ik}$ with smaller
   feasible regions whose union does not contain $(x^i, y^i)$, but contains
   all the solutions of $LP_i$ with $x \in \mathbb{Z}$, by choosing a variable
   to constrain.\;
  Add the corresponding new nodes $N_{i1}, N_{ik}$ to $\mathcal{L}$ and go to step 2.
  }
  
  Return $\ubar z$\;
  }
\caption{Branch and Bound algorithm for Mixed-Integer Programming}
\end{algorithm}


While finding optimal solutions with Branch and Bound is in worst case
 exponential-time, for some problems there exist Approximation Algorithms (AAs)
 which can provide feasible solutions in as fast as polynomial-time.
To achieve this, AAs exchange a guarantee on optimality for a guarantee the
 *quality* of the solution, within some factor of the optimal value.

An $\alpha$-approximation is an AA that guarantees that the solution value will
 always be within a constant factor $\alpha < 1$ of the optimal solution; i.e. for
 an approximate solution value $z_{A}$, it must be that
 $\alpha z \leq z_{A} \leq z$.
As a result, for a maximisation problem, we can analytically derive an upper
 bound on the optimal value from the lower bound guarantee on the optimal
 solution by simple algebraic manipulation. 
In this way, we can use AAs in lieu of an LP relaxation to obtain valid dual
 bounds on our optimal solution. 
Because we are leveraging guarantees on the optimal value, the dual bounds we
 obtain are also guaranteed to be within a level of accuracy. 
This is distinct from LP relaxations, which can provide arbitrarily poor dual
 bounds. 

When conducting a Branch and Bound with AAs, an important observation is that
 the general strategies which have contributed to the success of the
 standard Branch and Bound with LPs must be addressed. 
In particular, methods of branching and node selection do not have a clear
 translation with respect to AAs. 
In this project, we investigate methods of effectively using dual bounds
 provided by AAs for the 0,1 Knapsack. 
To do this, we investigate known approximation schemes and methods to construct
 high quality dual bounds from them, as well as devising branching strategies
 within its Branch and Bound. 

# Literature Review

We now give a brief literature review of the research and improvements to the
 Branch and Bound approach, as well as an overview of details and approaches
 with AAs that are central to the premise of this project.

## Branch and Bound improvements and strategies

While our previous description of the Branch and Bound approach captured the
 core components, there still remain many aspects which are uncertain: which
 variables to constrain for branching, alternative methods to relax a solution,
 and node selection schemes. 
These problems are in fact non-trivial and represent active research areas
 within Mixed-Integer Programming. 
In light of this, we provide an overview of known strategies and the body of
 research that comprises the current understanding in the field.

### Branching Strategies

Deriving useful subproblems is central to the Branch and Bound approach, as it
 provides us with the partial enumeration required to establish bounds on an
 optimal solution value. 
A basic strategy is to branch on the variable with the largest fractional
 component, however \cite{achterberg2005branching} has shown this to be as bad
 as random selection. 
It is possible to make inferences based on the properties of each variable's
 fractional component, as shown by \cite{tomlin1970branch} by the proposal of "use penalties".
This was used to inform the branching process
 where "up penalties" and "down penalties" were calculated for each variable
 based on their respective fractional components, after which the variable with
 the highest penalty in any direction was chosen.
However, \cite {mitra1973investigation} showed with computational experiments that such approaches may
 have limited use, while approaches involving issuing "pseudocosts" as proposed
 by \cite{benichou1971experiments} may have advantages. 

In an attempt to provide a categorisation for branching methods, 
\cite{morrison2016branch} proposed two groupings: *binary*/*non-binary* strategies,
 and *wide* strategies. 
Both problem types and tree depth provided the ideas central to these
 classifications.

A detailed survey of the literature and the current open questions in research
 is also provided by \cite{morrison2016branch}, where the reader is directed for
 further reading.

### Node selection schemes
Talk about best bound?

### Bounding optimal solutions

As we have seen, a natural method of finding dual bound on optimal solutions can
 be done with LP relaxations. 
Solving LPs has been efficiently possible since the introduction of Dantzig's
 Simplex method \cite{dantzig1951maximization}. 
Despite its efficiency, though, LP relaxations can provide arbtirarily poor
 bounds in this context: such an approach for finding the Vertex Cover of a
 graph will be generally far from optimal (CITE?). 
However, this is only one of many known ways to obtain valid dual bounds within
 Mixed Integer Programming: other examples include Semidefinite Programming
 relaxations (see \cite{lovasz1991cones,vandenberghe1996semidefinite}), and
 Langrangian relaxations \cite{geoffrion1974lagrangean}.


### Extensions, improvements, and related techniques
MIP solvers in the state-of-the-art often combine Branch and Bound strategies
 with various extensions to improve performance. 
One common extension to the method includes using cutting planes
 \cite{gomory1958outline} to produce the Branch-and-Cut algorithm. 
The cutting plane approach, introduced by \cite{dantzig1954solution}, and
 extended to general Integer Programs (IPs) in \cite{gomory1958outline},
 involves solving a LP relaxation of a program and formulates a constraint, a
 cutting plane, that separates this solution from the rest of the search space. 
A Branch and Cut approach \cite{padberg1987optimization, padberg1991branch},
 then, is a Branch and Bound with the ability to choose to add a cutting plane
 instead of branching on variable. 
For a more detailed treatment on this topic, the reader is referred to
 \cite{conforti2014integer}. 
Other key methods for extending the Branch and Bound are with presolving
 techniques \cite{mahajan2010presolving} and using primal heuristics
 \cite{berthold2006primal}.

Another related concept is the idea of warm starting a solution in linear
 programming linked to integer programming by \cite{ralphs2006duality}. 
In short, warm starting is a method to exploit information gained about the
 problem from previous computations to inform future ones. 
Ones simple method for this is to use previous computation in finding a starting
 primal bound in a Branch and Bound.

## Approximation algorithms for the Knapsack Problem

As mentioned, the case study chosen for experimentation within this project is
 the 0,1 Knapsack Problem (KP). 
Known to be NP-Hard (CITE), KP has been studied extensively both in to solving
 optimally and approximately (see, for example \cite{kellerer2005knapsack, horowitz1974computing,ingargiola1973reduction,kolesar1967branch,garfinkel1972integer,nemhauser1969discrete}), and as such, it serves well as a topic for
 analysis. 
We describe the formulation for KP as follows: given $n$ items associated
 weights $w_i$ and values $v_i$, and Knapsack capacity $W$, our program is

\begin{equation*}
\begin{array}{ll@{}ll}
\text{minimize}  & \displaystyle\sum\limits_{i=1}^{n} v_{i}&x_{i} &\\
\text{subject to}& \displaystyle\sum\limits_{i=1}^{n}     &w_{i} x_{i} \leq W, 
                                                \text{ and } x_{i} \in \{0,1\}\\
                 &  
\end{array}
\end{equation*}

It is well known that KP lends itself well to a Dynamic Programming (DP)
 approach, although there are many ways to conduct this. 
One of the most well known examples, performing a DP by values as presented by
 \cite{vazirani2013approximation}, can find a solution in $O(n^2 P)$ time for
 the most profitable item's value $P$, which we now describe: let $A(i, p)$ be
 the minimal weight of the solution to KP with only the first $i$ items
 available, where the total value is exactly $p$. 
If no such set exists\footnote{that is, the first $i$ items cannot yield a
 value of $p$ and so cannot have an associated minimal weight}, then
 $A(i,p) = \infty$. 

The DP recurrence can then be defined as follows:

\begin{equation}
  A(i+1,p) = \begin{cases}
    \text{min} \{A(i,p), w_{i+1} + A(i,p-v_{i+1})\} \text{, if }v_{i+1}<p \\
    A(i+1, p) = A(i,p) \text{ otherwise}
  \end{cases}
\end{equation}

The optimal solution value can then be found be finding the value
 max$\{p$ $|$ $A(n. p)$ $\leq$ $W\}$. 

Being NP-Hard, KP does not admit a polynomial time algorithm, however we can
 construct a *polynomial time approximation scheme* (PTAS). 
A PTAS is guaranteed to give a solution value within a factor of $(1-\epsilon)$
 of optimality, and have running time bounded by a polynomial in $n$, for fixed
 values of $\epsilon$.
A classic example of a PTAS was presented by (cite Sahni), with time complexity
 $O(n^{1 / \epsilon})$. 

If we restrict the definition of a PTAS further, so that the running time is
 bounded by a polynomial in both $n$ and $\epsilon$, we obtain a *fully
 polynomial time approximation scheme* (FPTAS). 
This means we can choose an $\epsilon$ that gives us a high quality solution
 while still being polynomially bounded. 

It is common for FPTAS's for KP to to achieve polynomially bounded run time by
 adjusting the input problem to reduce its complexity, and using a DP to
 then solve this adjusted problem. 
As such, the construction of an FPTAS normally involves two distinct steps:
 a preprocessing step to form the simpler problem, and a step where this is
 solved to find an approximate value for the original problem.
For our ends, we consider the examples within the literature which have led to
 what are now textbook examples of FPTAS's to show the performance of well
 established approximations when used in a Branch and Bound scheme.

### Ibarra and Kim's FPTAS (1975) 

What is now a standard method of constructing an FPTAS for KP was first
 described by Ibarra and Kim \cite{ibarra1975fast}. 
While there are several components at play in their proposal algorithm,
 the core of what creates the approximation involves a scaling the values
 of all items down to by a factor dependant on $\epsilon$. 
The distillation of their algorithm, as described by
 \cite{vazirani2013approximation}, is presented below:

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A KP problem instance, and error parameter $\epsilon$.}
  \KwResult{The Weight and value pair of the optimal solution}
  Given $\epsilon$, let $K = \frac{\epsilon P}{n}$\;
  For each object $i$, define adjusted values $v'=\left \lfloor \frac{v_i}{K}\right \rfloor$\;
  Run a DP with these adjusted item values\;
  Return the approximation solution set, $S'$, provided by the DP\;
\caption{FPTAS for Knapsack}
\end{algorithm}

By using the DP recurrence (1) described in the previous section, we can obtain an
 FPTAS for the Knapsack.
We now present the argument for this as described by \cite{vazirani2013approximation}.

\begin{lemma}
The set, $S'$, output by the algorithm satisfies:
$$\text{value}(S') \geq (1-\epsilon) \cdot OPT$$
\end{lemma}

\begin{proof}

 Let $z_{S'}$ be the value of the solution set $S'$ obtained from the FPTAS, and
 $z_O$ be the value of the optimal solution set $O$. 
 Also, let $z'_{S'}$ be the value of the solution set with adjusted/truncated
 values value the FPTAS's solution set $S'$, and $z'_O$ be the optimal solution
 set with similarly adjusted value for all items.  

Because of the flooring operation truncating off at most $K$ we know that the
 difference between $z_{O}$ and $K \cdot z'_{O}$ would be at most $nK$.
 Furthermore, since $z'_{S'}$ is the optimal value for the adjusted profits,
 $z'_{O}$ cannot exceed it.

 So, 

$$z_{O} - nK \leq K \cdot z'_O \leq K \cdot z'_{S'} \leq z_{S'} \leq z_O$$ 

Since $K = \frac{\epsilon P}{K}$, 

$$z_O - nK = z_O - \epsilon P$$ 

Finally, since $P \leq z_O$,

$$(1-\epsilon)\cdot z_O \leq z_{S'} \leq z_O$$

\end{proof}

\begin{theorem}
Algorithm 2 is an FPTAS for KP
\end{theorem}

\begin{proof}
Assume that it is the case. Therefore by Proof By Assumption we have proven it
 to be true.
\end{proof}

This constitutes the core of what is one of two textbook approaches to
 constructing an FPTAS for KP. 
There are nuanced optimisations which supplement this original algorithm's
 approach given by \cite{ibarra1975fast}, however for simplicity we omit
 such details here.

### Lawler (1978)

The second standard construction for obtaining an FPTAS for KP was presented
 originally by \cite{lawler1979fast}, which we describe here as it was
 illustrated in \cite{williamson2011design}.
Building on the contributions by \cite{ibarra1975fast}, this approach describes
 an alternative FPTAS which still maintains the basic approach shown in
 Algorithm 2. 
Specifically, we construct this algorithm by providing a different DP algorithm.

For $j \leq n$, let $A_j$ be an array which contains representations of
 solutions to KP for the first $j$ items.
A solution is represented by an ordered pair $(v, w)$ in $A_j$ if there is a
 set of the first $j$ items with value $v$ and weight $w \leq W$. 
Instead of explicitly storing all partial solutions, we resolve to only store
 partial solutions which are not *dominated* by other partial solutions. 
We say a solution $(v,w)$ dominates solution $(v', w')$ if $v \geq v'$ and
 $w \leq w'$. 

The dynamic programming algorithm, as presented by \cite{williamson2011design},
 then follows:

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A KP problem instance}
  \KwResult{The weight and value pair of the optimal solution}
  
  \For{$j\gets2$ to $n$}{
    \For{POO}{
      Shit!\; 
      }
    }

\caption{Alternative dynamic programming for Knapsack}
\end{algorithm}



# Design and Analysis

In light of the approaches described in the literature to both Branch and Bound
 and AAs, our objective is to now find a method of constructing high-quality
 dual bounds. 
Intuitively, the guarantees which help define given FPTAS's for KP are not
 proven with such applications in mind, so there are potential extensions to
 the analysis which we can use to find stronger dual bounds. 
In this section, we demonstrate analysis methods which have been undertaken
 to this end. 

## Analytic derivation of dual bounds a priori

To form a basis to our discussion, we reintroduce the bounds obtained by the
 FPTAS originally presented by Ibarra and Kim in \cite{ibarra1975fast}. 
In particular, recall that we found

$$(1-\epsilon) \cdot z_O \leq z_{S'} \leq z_O$$


Which means that before running the approximation we know *a priori* the range
 for the optimal value:

$$z_O \in [z_{S'}, \frac{z'_{S'}}{1-\epsilon}]$$

This provides us with the following first attempt at an amendeded Branch and
 Bound algorithm:
\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{A MIP instance, and an AA for the given problem.}
  \KwResult{The Weight and value pair of the optimal solution}
  Let $\bar z :=$ approximate solution\;
  \While{$\mathcal{L}$ is not empty}
  {
  Choose a node $N_i$ from $\mathcal{L}$, and delete it from $\mathcal{L}$\;
  Run the AA, deriving upper bound (UB) and lower bound (LB) in the process\;
  \If{UB $\leq \bar z$}
  {
  Go to step 3\;
  }
  \Else
  {
  \If{LB$>\bar z$}
  {
  $\bar z := $ LB\;
  }
  }
  \If{for $N_i$'s parent's UB, $UB_p$, $UB > UB_p$}
  {
  Set $UB := UB_p$
  }
  \Else
  {
  Choose a variable to branch on, then generate and enqueue new child nodes;\
  }
  Return $\ubar z$\;
  }
\caption{Branch and Bound with Approximation Algorithms}
\end{algorithm}

## Derivation of dual bounds a posteriori

We can obtain improvements to our dual bounds by observing information obtained
 following completion of the FPTAS, however, i.e. *a posteriori*.
To do this, we first observe that the approximate solution set $S'$
 will have a profit of $p(S')$. 
This value will be the value of the adjusted profits $p'(S')$ scaled up by $K$,
 plus any profit $\omega$ lost from the flooring operation, i.e.

\begin{align*}
  p(S') &= Kp'(S')+\omega
\end{align*}

**Todo the same analysis but with omega involves**.

This analysis leads to the insight that 
$$OPT \leq \frac{(p(S') \omega)}{1-\epsilon}$$

Which gives us 

$$OPT \leq p(S') + Kn - \omega $$


## Improved a posteriori dual bounds
Another approach is if we round up


## Truncation branching strategy

# Implementation and results

## Methods for implemnetation

Benchmarks lol

## What is a logical way to lay all this out?

# Todos
(**Put warm starting in future work**)

\bibliography{project_overview_draft}
\bibliographystyle{plain}
